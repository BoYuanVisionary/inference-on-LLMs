# controllable text generation, works published this year (no publication on nips and colm)

- **Improving Open-Ended Text Generation via Adaptive Decoding**  
 This study introduces adaptive decoding, a mechanism that dynamically empowers language models to ascertain a sensible candidate set during generation. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. ![](https://img.shields.io/badge/ICML-orange)
  
- **Successor Features for Efficient Multi-Subject Controlled Text Generation**  
 This method leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. ![](https://img.shields.io/badge/ICML-orange)
  
- **Model-Based Minimum Bayes Risk Decoding for Text Generation**  
  We propose a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. ![](https://img.shields.io/badge/ICML-orange)

- **Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation**  
  We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce a training framework incorporating meta-learning.![](https://img.shields.io/badge/ACL-orange)
  
