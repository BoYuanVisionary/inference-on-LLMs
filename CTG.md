# controllable text generation, works published this year

- **Improving Open-Ended Text Generation via Adaptive Decoding**  
 This study introduces adaptive decoding, a mechanism that dynamically empowers language models to ascertain a sensible candidate set during generation. Specifically, we introduce an entropy-based metric called confidence and conceptualize determining the optimal candidate set as a confidence-increasing process. ![](https://img.shields.io/badge/ICML-orange)
  
- **Successor Features for Efficient Multi-Subject Controlled Text Generation**  
 This method leverages the concept of successor features to decouple the dynamics of LLMs from task-specific rewards. By employing successor features, our method proves to be memory-efficient and computationally efficient for both training and decoding, especially when dealing with multiple target subjects. ![](https://img.shields.io/badge/ICML-orange)
  
- **Model-Based Minimum Bayes Risk Decoding for Text Generation**  
  We propose a variant of MBR that uses the model probability itself as the estimate of the probability distribution instead of the Monte Carlo estimate. ![](https://img.shields.io/badge/ICML-orange)


  
